{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6265331-bf34-4f40-89b9-3b6037181399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import pyttsx3\n",
    "import pytesseract\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# Set up Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Initialize Text-to-Speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize Google Generative AI LLM\n",
    "GEMINI_API_KEY = \"Your-API-Key-Here\"  # Replace with your API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", api_key=GEMINI_API_KEY)\n",
    "\n",
    "# App Layout and Design\n",
    "st.set_page_config(page_title=\"Aid\", layout=\"centered\", page_icon=\"\")\n",
    "st.title(\"Aid - AI Assistive Tool\")\n",
    "st.markdown(\"*Helping visually impaired users with AI-powered image analysis.*\")\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.header(\"Features\")\n",
    "st.sidebar.markdown(\n",
    "    \"\"\"\n",
    "- *üîç Describe Scene*: AI-powered insights about uploaded images.\n",
    "- *üìù Extract Text*: OCR to read text in images.\n",
    "- *üîä Text-to-Speech*: Listen to the extracted text.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "st.sidebar.info(\"Upload an image to get started!\")\n",
    "\n",
    "# File uploader for user input\n",
    "uploaded_file = st.file_uploader(\n",
    "    \"Upload an image (JPG, PNG, JPEG)\", type=[\"jpg\", \"jpeg\", \"png\"]\n",
    ")\n",
    "\n",
    "# Helper Functions\n",
    "def extract_text_from_image(image):\n",
    "    \"\"\"Extract text using Tesseract OCR.\"\"\"\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def text_to_speech(text):\n",
    "    \"\"\"Convert extracted text to speech.\"\"\"\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def generate_scene_description(prompt, image_data):\n",
    "    \"\"\"Generate a scene description using Google Gemini API.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n\\nHere is the image data: {image_data[:500]}...\"\n",
    "    response = llm.predict(text=full_prompt)\n",
    "    return response\n",
    "\n",
    "# Process Uploaded Image\n",
    "if uploaded_file:\n",
    "    image = Image.open(uploaded_file)\n",
    "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    # Convert uploaded file into bytes for LLM processing\n",
    "    image_bytes = uploaded_file.read()\n",
    "\n",
    "    # Features Buttons\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "    # Scene Description\n",
    "    if col1.button(\"üîç Describe Scene\"):\n",
    "        with st.spinner(\"Generating scene description...\"):\n",
    "            try:\n",
    "                input_prompt = \"\"\"\n",
    "                You are an AI assistant helping visually impaired individuals by describing the scene in the uploaded image. \n",
    "                Provide:\n",
    "                1. A list of visible items with their purposes.\n",
    "                2. An overall description of the image.\n",
    "                3. Safety precautions or tips for navigating the scene.\n",
    "                \"\"\"\n",
    "                scene_description = generate_scene_description(input_prompt, image_bytes)\n",
    "                st.subheader(\"üîç Scene Description\")\n",
    "                st.write(scene_description)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error generating scene description: {e}\")\n",
    "\n",
    "    # Text Extraction\n",
    "    if col2.button(\"üìù Extract Text\"):\n",
    "        with st.spinner(\"Extracting text from the image...\"):\n",
    "            text = extract_text_from_image(image)\n",
    "            if text.strip():\n",
    "                st.subheader(\"üìù Extracted Text\")\n",
    "                st.text_area(\"Text Content\", text, height=200)\n",
    "            else:\n",
    "                st.warning(\"No text detected in the image.\")\n",
    "\n",
    "    # Text-to-Speech\n",
    "    if col3.button(\"üîä Text-to-Speech\"):\n",
    "        with st.spinner(\"Converting text to speech...\"):\n",
    "            text = extract_text_from_image(image)\n",
    "            if text.strip():\n",
    "                text_to_speech(text)\n",
    "                st.success(\"‚úÖ Text-to-Speech conversion completed!\")\n",
    "            else:\n",
    "                st.warning(\"No text available for conversion.\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "¬© 2024 VisionAid | Built with ‚ù§ using Streamlit, Google Generative AI, and Python.  \n",
    "Empowering accessibility through technology.\n",
    "\"\"\"\n",
    ")\n",
    "streamlit==1.18.0\n",
    "Pillow==9.4.0\n",
    "pyttsx3==2.90\n",
    "pytesseract==0.3.10\n",
    "google-generativeai==0.0.3\n",
    "langchain-google-genai==0.0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
